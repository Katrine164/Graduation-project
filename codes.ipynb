{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "\n",
        "csv_file = os.path.join(os.getcwd(), \"talabat_15_restaurants_reviews.csv\")\n",
        "\n",
        "# Map restaurant names to branch IDs (extracted from your URLs)\n",
        "restaurants = {\n",
        "    \"99 Grill\": 624149,\n",
        "    \"Pizza Hut\": 600352,\n",
        "    \"Ninja Sushi\": 734783,\n",
        "    \"Wazzup Dog\": 628339,\n",
        "    \"Between Buns\": 40177,\n",
        "    \"Dominos Pizza\": 9778,\n",
        "    \"Shawerma Reem\": 650859,\n",
        "    \"Shawarmaati\": 47347,\n",
        "    \"Mr Hotdog\": 661804,\n",
        "    \"Xn Shawerma\": 736547,\n",
        "    \"Burger Maker\": 683243,\n",
        "    \"Ibra Sandwich\": 760475,\n",
        "    \"Crispy Chicken\": 662822,\n",
        "    \"Buffalo Wings Rings\": 638830,\n",
        "    \"Chicken Kingdom\": 730696\n",
        "}\n",
        "\n",
        "all_reviews = []\n",
        "\n",
        "for name, branch_id in restaurants.items():\n",
        "    print(f\"Scraping reviews for {name} (branch {branch_id})...\")\n",
        "\n",
        "    page = 1\n",
        "    page_size = 50  # number of reviews per request\n",
        "    total_pages = 1  # will update from first request\n",
        "\n",
        "    while page <= total_pages:\n",
        "        url = f\"https://www.talabat.com/nextFeedbackApi/branches/{branch_id}/reviews/{page}/{page_size}\"\n",
        "        headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0\",\n",
        "            \"accept\": \"application/json, text/plain, /\"\n",
        "        }\n",
        "        response = requests.get(url, headers=headers)\n",
        "        data = response.json()\n",
        "\n",
        "        # Update total pages from API response\n",
        "        total_pages = data.get(\"totalPages\", 1)\n",
        "\n",
        "        # Extract reviews\n",
        "        for review in data.get(\"details\", []):\n",
        "            all_reviews.append({\n",
        "                \"restaurant_name\": name,\n",
        "                \"review_date\": review.get(\"date\"),\n",
        "                \"rating\": review.get(\"rate\"),\n",
        "                \"review_text\": review.get(\"review\")\n",
        "            })\n",
        "\n",
        "        page += 1\n",
        "        time.sleep(random.uniform(0.5, 1.5))  # polite delay\n",
        "\n",
        "# Save CSV\n",
        "csv_file = os.path.join(os.getcwd(), \"talabat_15_restaurants_reviews.csv\")\n",
        "keys = all_reviews[0].keys()\n",
        "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    dict_writer = csv.DictWriter(f, keys)\n",
        "    dict_writer.writeheader()\n",
        "    dict_writer.writerows(all_reviews)\n",
        "\n",
        "print(f\"\\n Successfully saved {len(all_reviews)} reviews to CSV!\")\n",
        "print(f\"File location: {csv_file}\")"
      ],
      "metadata": {
        "id": "dW1EJtOwwKft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import emoji\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import numpy as np\n",
        "\n",
        "# ------------------------\n",
        "# 1. Load data\n",
        "# ------------------------\n",
        "df = pd.read_csv(r\"C:\\Users\\Kat\\Downloads\\talabat_15_restaurants_reviews.csv\", encoding='utf-8')\n",
        "\n",
        "# ------------------------\n",
        "# 2. Preprocessing\n",
        "# ------------------------\n",
        "def preprocess(text):\n",
        "    text = str(text)\n",
        "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
        "    return text\n",
        "\n",
        "df['cleaned_text'] = df['review_text'].apply(preprocess)\n",
        "\n",
        "# ------------------------\n",
        "# 3. Load model\n",
        "# ------------------------\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, output_attentions=True)\n",
        "model.eval()\n",
        "\n",
        "labels = ['1_star', '2_star', '3_star', '4_star', '5_star']\n",
        "\n",
        "# ------------------------\n",
        "# 4. Prediction function\n",
        "# ------------------------\n",
        "def predict_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "    idx = np.argmax(probs)\n",
        "    stars = idx + 1  # Convert 0–4 to 1–5\n",
        "    return stars, probs[idx]\n",
        "\n",
        "df[['model_star', 'model_confidence']] = df['cleaned_text'].apply(lambda x: pd.Series(predict_sentiment(x)))\n",
        "\n",
        "# ------------------------\n",
        "# 5. Combine rating with model\n",
        "# ------------------------\n",
        "# Map 1–5 stars to sentiment score (-1 to +1)\n",
        "rating_map = {1: -1, 2: -0.5, 3: 0, 4: 0.5, 5: 1}\n",
        "df['rating_score'] = df['rating'].map(rating_map)\n",
        "df['model_score'] = df['model_star'].map(rating_map) * df['model_confidence']\n",
        "\n",
        "# Weighted final score\n",
        "df['final_score'] = 0.7 * df['model_score'] + 0.3 * df['rating_score']\n",
        "\n",
        "def map_final_sentiment(score):\n",
        "    if score > 0.2:\n",
        "        return \"positive\"\n",
        "    elif score < -0.2:\n",
        "        return \"negative\"\n",
        "    else:\n",
        "        return \"neutral\"\n",
        "\n",
        "df['final_sentiment'] = df['final_score'].apply(map_final_sentiment)\n",
        "\n",
        "# ------------------------\n",
        "# 6. Extract top words using attention weights\n",
        "# ------------------------\n",
        "def get_top_attention_words(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        attentions = outputs.attentions  # List of attention layers\n",
        "\n",
        "    attn = attentions[-1][0].mean(dim=0)  # last layer, average heads\n",
        "    cls_attention = attn[0].cpu().numpy()  # CLS token attention\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "    token_scores = list(zip(tokens, cls_attention))\n",
        "    token_scores = [t for t in token_scores if t[0] not in tokenizer.all_special_tokens]\n",
        "    token_scores = sorted(token_scores, key=lambda x: x[1], reverse=True)\n",
        "    top_tokens = [t for t, s in token_scores[:5]]\n",
        "    return \" \".join(top_tokens)\n",
        "\n",
        "df['top_words'] = df['cleaned_text'].apply(get_top_attention_words)\n",
        "\n",
        "# ------------------------\n",
        "# 7. Save final CSV\n",
        "# ------------------------\n",
        "df.to_csv(r\"C:\\Users\\Kat\\Documents\\talabat_restaurants_final.csv\", index=False, encoding='utf-8')\n",
        "print(\" CSV saved with sentiment\")"
      ],
      "metadata": {
        "id": "HL2j8yjKwl5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from deep_translator import GoogleTranslator\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# -----------------------------\n",
        "# Setup\n",
        "# -----------------------------\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "EN_STOP = set(stopwords.words(\"english\"))\n",
        "AR_STOP = set([\n",
        "    \"مش\",\"مو\",\"ما\",\"في\",\"على\",\"من\",\"عن\",\"كان\",\"كانت\",\"جدا\",\"جداً\",\"مره\",\"مرة\"\n",
        "])\n",
        "\n",
        "translator = GoogleTranslator(source=\"auto\", target=\"en\")\n",
        "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "\n",
        "# -----------------------------\n",
        "# Load data\n",
        "# -----------------------------\n",
        "df = pd.read_csv(\n",
        "    r\"C:\\Users\\Kat\\Downloads\\talabat_restaurants_final.csv\",\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Text cleaning\n",
        "# -----------------------------\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "df[\"cleaned_text\"] = df[\"cleaned_text\"].apply(clean_text)\n",
        "\n",
        "# -----------------------------\n",
        "# Translate Arabic → English\n",
        "# -----------------------------\n",
        "def translate_if_needed(text):\n",
        "    try:\n",
        "        if re.search(r\"[\\u0600-\\u06FF]\", text):\n",
        "            return translator.translate(text)\n",
        "        return text\n",
        "    except:\n",
        "        return text\n",
        "\n",
        "df[\"en_text\"] = df[\"cleaned_text\"].apply(translate_if_needed)\n",
        "\n",
        "# -----------------------------\n",
        "# Split into sentences\n",
        "# -----------------------------\n",
        "def split_sentences(text):\n",
        "    return [s.strip() for s in re.split(r\"[.!?]\", text) if len(s.strip()) > 3]\n",
        "\n",
        "df[\"sentences\"] = df[\"en_text\"].apply(split_sentences)\n",
        "\n",
        "# -----------------------------\n",
        "# Define semantic categories\n",
        "# -----------------------------\n",
        "CATEGORIES = {\n",
        "    \"Food Temperature\": [\n",
        "        \"food was cold\", \"not hot\", \"arrived cold\", \"temperature bad\"\n",
        "    ],\n",
        "    \"Taste & Flavor\": [\n",
        "        \"bad taste\", \"not tasty\", \"delicious\", \"flavorless\"\n",
        "    ],\n",
        "    \"Cooking Quality\": [\n",
        "        \"undercooked\", \"raw\", \"burnt\", \"dry food\", \"not cooked well\"\n",
        "    ],\n",
        "    \"Missing / Wrong Items\": [\n",
        "        \"missing item\", \"order incomplete\", \"wrong order\"\n",
        "    ],\n",
        "    \"Portion Size\": [\n",
        "        \"small portion\", \"quantity too small\"\n",
        "    ],\n",
        "    \"Packaging\": [\n",
        "        \"bad packaging\", \"spilled\", \"leaking\"\n",
        "    ],\n",
        "    \"Delivery Speed\": [\n",
        "        \"late delivery\", \"fast delivery\", \"on time\"\n",
        "    ],\n",
        "    \"Service\": [\n",
        "        \"bad service\", \"good service\", \"rude staff\", \"polite staff\"\n",
        "    ],\n",
        "    \"Price / Value\": [\n",
        "        \"expensive\", \"not worth\", \"good price\"\n",
        "    ],\n",
        "    \"Cleanliness\": [\n",
        "        \"dirty\", \"not clean\", \"clean restaurant\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Encode category examples\n",
        "category_embeddings = {}\n",
        "for cat, examples in CATEGORIES.items():\n",
        "    category_embeddings[cat] = model.encode(examples, convert_to_tensor=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Assign sentence to category\n",
        "# -----------------------------\n",
        "def classify_sentence(sentence):\n",
        "    sent_emb = model.encode(sentence, convert_to_tensor=True)\n",
        "    best_cat = None\n",
        "    best_score = 0\n",
        "\n",
        "    for cat, emb in category_embeddings.items():\n",
        "        score = util.cos_sim(sent_emb, emb).max().item()\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_cat = cat\n",
        "\n",
        "    return best_cat if best_score >= 0.55 else None\n",
        "\n",
        "# -----------------------------\n",
        "# Collect categories per restaurant\n",
        "# -----------------------------\n",
        "restaurant_results = {}\n",
        "\n",
        "for restaurant, group in df.groupby(\"restaurant_name\"):\n",
        "    neg_counts = Counter()\n",
        "    pos_counts = Counter()\n",
        "\n",
        "    total_neg = 0\n",
        "    total_pos = 0\n",
        "\n",
        "    for _, row in group.iterrows():\n",
        "        sentiment = row[\"final_sentiment\"]\n",
        "        for sent in row[\"sentences\"]:\n",
        "            category = classify_sentence(sent)\n",
        "            if not category:\n",
        "                continue\n",
        "\n",
        "            if sentiment == \"negative\":\n",
        "                neg_counts[category] += 1\n",
        "                total_neg += 1\n",
        "            elif sentiment == \"positive\":\n",
        "                pos_counts[category] += 1\n",
        "                total_pos += 1\n",
        "\n",
        "    # Convert to percentages\n",
        "    neg_percent = {\n",
        "        k: round((v / total_neg) * 100, 1)\n",
        "        for k, v in neg_counts.items()\n",
        "    } if total_neg > 0 else {}\n",
        "\n",
        "    pos_percent = {\n",
        "        k: round((v / total_pos) * 100, 1)\n",
        "        for k, v in pos_counts.items()\n",
        "    } if total_pos > 0 else {}\n",
        "\n",
        "    restaurant_results[restaurant] = {\n",
        "        \"top_problems\": \", \".join(\n",
        "            [f\"{k} ({v}%)\" for k, v in sorted(neg_percent.items(), key=lambda x: -x[1])[:5]]\n",
        "        ),\n",
        "        \"top_positive_features\": \", \".join(\n",
        "            [f\"{k} ({v}%)\" for k, v in sorted(pos_percent.items(), key=lambda x: -x[1])[:5]]\n",
        "        )\n",
        "    }\n",
        "\n",
        "# -----------------------------\n",
        "# Save final results\n",
        "# -----------------------------\n",
        "final_df = pd.DataFrame.from_dict(restaurant_results, orient=\"index\").reset_index()\n",
        "final_df.rename(columns={\"index\": \"restaurant_name\"}, inplace=True)\n",
        "\n",
        "final_df.to_csv(\n",
        "    r\"C:\\Users\\Kat\\Downloads\\talabat_restaurant_insights_percent.csv\",\n",
        "    index=False,\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "print(\"Final NLP-based restaurant insights saved successfully\")"
      ],
      "metadata": {
        "id": "ujRrEGTPwvpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ===============================\n",
        "# 1. Load raw reviews data\n",
        "# ===============================\n",
        "df = pd.read_csv(\n",
        "    r\"C:\\Users\\Kat\\Downloads\\talabat_restaurants_final.csv\",\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# 2. Convert review_date to datetime\n",
        "# ===============================\n",
        "df[\"review_date\"] = pd.to_datetime(df[\"review_date\"], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"review_date\"])\n",
        "\n",
        "# ===============================\n",
        "# 3. Create Year-Month column\n",
        "# ===============================\n",
        "df[\"year_month\"] = df[\"review_date\"].dt.to_period(\"M\").astype(str)\n",
        "\n",
        "# ===============================\n",
        "# 4. Create sentiment flags\n",
        "# ===============================\n",
        "df[\"is_positive\"] = (df[\"final_sentiment\"] == \"positive\").astype(int)\n",
        "df[\"is_negative\"] = (df[\"final_sentiment\"] == \"negative\").astype(int)\n",
        "df[\"is_neutral\"]  = (df[\"final_sentiment\"] == \"neutral\").astype(int)\n",
        "\n",
        "# ===============================\n",
        "# 5. Aggregate monthly timeline metrics\n",
        "# ===============================\n",
        "timeline_df = (\n",
        "    df.groupby([\"restaurant_name\", \"year_month\"])\n",
        "      .agg(\n",
        "          review_count=(\"final_sentiment\", \"count\"),\n",
        "          avg_final_score=(\"final_score\", \"mean\"),\n",
        "          positive_ratio=(\"is_positive\", \"mean\"),\n",
        "          negative_ratio=(\"is_negative\", \"mean\"),\n",
        "          neutral_ratio=(\"is_neutral\", \"mean\")\n",
        "      )\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# 6. Apply minimum review threshold\n",
        "# ===============================\n",
        "MIN_REVIEWS = 10\n",
        "timeline_df = timeline_df[timeline_df[\"review_count\"] >= MIN_REVIEWS]\n",
        "\n",
        "# ===============================\n",
        "# 7. Convert ratios to percentages\n",
        "# ===============================\n",
        "timeline_df[\"positive_ratio\"] = (timeline_df[\"positive_ratio\"] * 100).round(1)\n",
        "timeline_df[\"negative_ratio\"] = (timeline_df[\"negative_ratio\"] * 100).round(1)\n",
        "timeline_df[\"neutral_ratio\"]  = (timeline_df[\"neutral_ratio\"] * 100).round(1)\n",
        "timeline_df[\"avg_final_score\"] = timeline_df[\"avg_final_score\"].round(2)\n",
        "\n",
        "# ===============================\n",
        "# 8. Sort for proper timeline order\n",
        "# ===============================\n",
        "timeline_df = timeline_df.sort_values(\n",
        "    by=[\"restaurant_name\", \"year_month\"]\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# 9. Calculate month-to-month sentiment change\n",
        "# ===============================\n",
        "timeline_df[\"positive_change\"] = (\n",
        "    timeline_df.groupby(\"restaurant_name\")[\"positive_ratio\"]\n",
        "    .diff()\n",
        ")\n",
        "\n",
        "timeline_df[\"negative_change\"] = (\n",
        "    timeline_df.groupby(\"restaurant_name\")[\"negative_ratio\"]\n",
        "    .diff()\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# 10. Define spike/drop thresholds\n",
        "# ===============================\n",
        "DROP_THRESHOLD = -15   # -15% or worse\n",
        "SPIKE_THRESHOLD = 15   # +15% or better\n",
        "\n",
        "def classify_change(x):\n",
        "    if pd.isna(x):\n",
        "        return \"Stable\"\n",
        "    elif x <= DROP_THRESHOLD:\n",
        "        return \"Sentiment Drop\"\n",
        "    elif x >= SPIKE_THRESHOLD:\n",
        "        return \"Sentiment Spike\"\n",
        "    else:\n",
        "        return \"Stable\"\n",
        "\n",
        "# ===============================\n",
        "# 11. Classify sentiment events\n",
        "# ===============================\n",
        "timeline_df[\"sentiment_event\"] = timeline_df[\"positive_change\"].apply(classify_change)\n",
        "\n",
        "# ===============================\n",
        "# 12. Save final output\n",
        "# ===============================\n",
        "timeline_df.to_csv(\n",
        "    r\"C:\\Users\\Kat\\Downloads\\restaurant_Timeline_analysis_drop.csv\",\n",
        "    index=False,\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "print(\" Timeline analysis + sentiment spike/drop detection completed\")\n",
        "print(\" File saved: restaurant_sentiment_events.csv\")"
      ],
      "metadata": {
        "id": "3tQE8JTOw0nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# ===============================\n",
        "# 1. Load original sentiment file\n",
        "# ===============================\n",
        "df = pd.read_csv(\n",
        "    talabat_restaurants_final.csv\",\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# 2. Prepare dates\n",
        "# ===============================\n",
        "df[\"review_date\"] = pd.to_datetime(df[\"review_date\"], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"review_date\"])\n",
        "df[\"year_month\"] = df[\"review_date\"].dt.to_period(\"M\").astype(str)\n",
        "\n",
        "# ===============================\n",
        "# 3. Keep ONLY negative reviews\n",
        "# ===============================\n",
        "df = df[df[\"final_sentiment\"] == \"negative\"]\n",
        "\n",
        "# ===============================\n",
        "# 4. Define candidate problem categories\n",
        "# ===============================\n",
        "problem_categories = [\n",
        "    \"Food Temperature\",\n",
        "    \"Taste & Flavor\",\n",
        "    \"Service\",\n",
        "    \"Portion Size\",\n",
        "    \"Missing / Wrong Items\",\n",
        "    \"Cleanliness\",\n",
        "    \"Delivery Speed\",\n",
        "    \"Packaging\",\n",
        "    \"Price / Value\",\n",
        "    \"Incorrect Billing\",\n",
        "    \"Other\"\n",
        "]\n",
        "\n",
        "# ===============================\n",
        "# 5. Load zero-shot classifier\n",
        "# ===============================\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# ===============================\n",
        "# 6. Detect problems for each review\n",
        "# ===============================\n",
        "def detect_problems(text, categories, threshold=0.2):\n",
        "    if not isinstance(text, str) or text.strip() == \"\":\n",
        "        return []\n",
        "    result = classifier(text, candidate_labels=categories, multi_label=True)\n",
        "    # Only keep labels above threshold\n",
        "    detected = [label for label, score in zip(result['labels'], result['scores']) if score >= threshold]\n",
        "    return detected\n",
        "\n",
        "# Apply detection\n",
        "df[\"detected_problem\"] = df[\"review_text\"].apply(lambda x: detect_problems(x, problem_categories))\n",
        "df = df.explode(\"detected_problem\")\n",
        "df = df[df[\"detected_problem\"].notna() & (df[\"detected_problem\"] != \"\")]\n",
        "\n",
        "# ===============================\n",
        "# 7. Aggregate problems monthly\n",
        "# ===============================\n",
        "problem_timeline = (\n",
        "    df.groupby([\"restaurant_name\", \"year_month\", \"detected_problem\"])\n",
        "      .size()\n",
        "      .reset_index(name=\"problem_count\")\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# 8. Convert to percentage\n",
        "# ===============================\n",
        "total_per_month = (\n",
        "    problem_timeline.groupby([\"restaurant_name\", \"year_month\"])[\"problem_count\"]\n",
        "    .transform(\"sum\")\n",
        ")\n",
        "\n",
        "problem_timeline[\"problem_percentage\"] = (\n",
        "    (problem_timeline[\"problem_count\"] / total_per_month) * 100\n",
        ").round(1)\n",
        "\n",
        "# ===============================\n",
        "# 9. Sort cleanly\n",
        "# ===============================\n",
        "problem_timeline = problem_timeline.sort_values(\n",
        "    by=[\"restaurant_name\", \"year_month\", \"problem_percentage\"],\n",
        "    ascending=[True, True, False]\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# 10. Save output\n",
        "# ===============================\n",
        "problem_timeline.to_csv(\n",
        "    restaurant_problem_timeline_monthly.csv\",\n",
        "    index=False,\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "print(\" Problem timeline analysis completed\")\n",
        "print(\" File saved: restaurant_problem_timeline_monthly.csv\")"
      ],
      "metadata": {
        "id": "9e1vO-5hw5El"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
